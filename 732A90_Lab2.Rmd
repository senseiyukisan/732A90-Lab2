---
title: "Question 2: Maximum Likelihood"
author: "Tim Yuki Washio"
date: "11/14/2020"
output: pdf_document
---

```{r setup, include=FALSE, q2=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(huxtable)
library(magrittr)
```

## 1. 

```{r data}
load("data.RData")
```

## 2.

We define the log-likelihood function as the logarithm of the likelihood function:

$$\mathcal{L}_x(\vartheta) = ln(L_x(\vartheta))$$
For normal distributions we define:

$$\mathcal{L}_x(\mu, \sigma^2) = -\frac{n}{2} ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum^n_{i = 1} (x_i-\mu)^2$$

For a given $n = 100$ we get:

$$\mathcal{L}_x(\mu, \sigma^2) = -\frac{100}{2} ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum^{100}_{i=1} (x_i-\mu)^2$$

We calculate the partial derivative of log-likelihood with respect to $\mu$ and set it equal to $0$:

$$\frac{\partial \mathcal{L}}{\partial \mu} = \frac{1}{\sigma^2} \sum^{100}_{i=1}(x_i-\mu) = 0 $$

We transform our equation and get: 

$$\hat{\mu} = \frac{1}{100} \sum^{100}_{i=1}x_i$$

Now we calculate the partial derivative of log-likelihood with respect to $\sigma$ and set it equal to $0$:

$$\frac{\partial \mathcal{L}}{\partial \sigma} = -\frac{100}{2\sigma^2} - \left[\frac{1}{2}\sum^{100}_{i=1}(x_i-\mu)^2\right]\left(- \frac{1}{(\sigma^2)^2} \right) = -\frac{100}{2\sigma^2} + \left[\frac{1}{2}\sum^{100}_{i=1}(x_i-\mu)^2\right] \frac{1}{(\sigma^2)^2} = \frac{1}{2\sigma^2}\left[\frac{1}{\sigma^2}\sum^{100}_{i=1}(x_i-\mu)^2-100\right] = 0$$

We transform our equation and get:

$$\hat{\sigma}^2 = \frac{1}{100}\sum^{100}_{i=1}(x_i-\mu)^2$$

```{r mean_variance, q2=TRUE}
mu_hat = sum(data)/length(data)
sigma_hat = sqrt(sum((data-mu_hat)^2)/length(data))

cat("mu_hat: ", mu_hat, "\nsigma_hat: ", sigma_hat)
```

## 3.

```{r minus_log_likelihood, q2=TRUE}
negative_log_likelihood <- function(param) {
  mu = param[1]
  sigma = param[2]
  n = length(data)
  log_likelihood = -n*0.5*log(2*pi*sigma^2)-(0.5/sigma^2)*sum((data-mu)^2)
  negative_log_likelihood = -(log_likelihood)
  return (negative_log_likelihood)
}

gradient <- function(params) {
  mu = params[1]
  sigma = params[2]
  n = length(data)
  result = c(-(sum(data)-n*mu)/sigma^2, (n/sigma)-(1/(sigma^3) * sum((data-mu)^2)))
  return(result)
}

cg_1 <- optim(par=c(0, 1), negative_log_likelihood, method="CG")
bfgs_1 <- optim(par=c(0, 1), fn=negative_log_likelihood, method="BFGS")
cg_2 <- optim(par=c(0, 1), fn=negative_log_likelihood, gr=gradient, method="CG")
bfgs_2 <- optim(par=c(0, 1), fn=negative_log_likelihood, gr=gradient, method="BFGS")
```


## 4.

```{r converge, q2=TRUE, echo=FALSE}
algorithms = rbind("CG", "CG", "BFGS", "BFGS")
params = rbind(cg_1$par, cg_2$par, bfgs_1$par, bfgs_2$par)
counts = rbind(cg_1$counts, cg_2$counts, bfgs_1$counts, bfgs_2$counts)
gradient = c("NO", "YES", "NO", "YES")
values = rbind(cg_1$value, cg_2$value, bfgs_1$value, bfgs_2$value)

df = data.frame(algorithms, params, counts, gradient, values)
colnames(df) = c("Algorithm", "Mean", "Var", "Function Counts", "Gradient Counts", "Gradient Specified", "Value")


pretty_df <-
  hux(df) %>%
  set_bold(row = 1, col = everywhere, value = TRUE) %>% 
  set_all_borders(TRUE)

set_number_format(pretty_df, 5)

```
